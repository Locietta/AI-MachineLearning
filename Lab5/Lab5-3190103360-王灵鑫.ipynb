{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595347945567",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练词向量word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 语料包预处理\n",
    "\n",
    "利用jieba进行分词操作（因为utf-8文件直接用jieba会乱码，所以转成了gbk）\n",
    "\n",
    "命令行执行如下命令预览分词效果\n",
    "\n",
    "``` cmd\n",
    "python -m jieba -d ' ' corpus10000.gbk > segmented.gbk\n",
    "```\n",
    "\n",
    "- 发现有很多中英文标点，无意义的空格以及广告的URL也被做了分词处理\n",
    "\n",
    "需要过滤掉这些无意义的文本\n",
    "\n",
    "### 1.1 滤过中英文标点\n",
    "\n",
    "- 新建文件cutword.dat,并在其中存入想滤过的标点(一行一个)\n",
    "- open+readlines读入\n",
    "- 在列表解析时加入if判断分词是否在过滤列表中\n",
    "\n",
    "### 1.2 滤过URL\n",
    "\n",
    "- 利用正则表达式对读入的文本先做删除处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Building prefix dict from the default dictionary ...\nLoading model from cache C:\\Users\\pc\\AppData\\Local\\Temp\\jieba.cache\nLoading model cost 0.678 seconds.\nPrefix dict has been built successfully.\n"
    }
   ],
   "source": [
    "# 导入必要的包\n",
    "import re\n",
    "import jieba\n",
    "\n",
    "def regex_filter(line):\n",
    "    url_regex = re.compile(r\"\"\"\n",
    "        (http?://)?\n",
    "        (https?://)?\n",
    "        (http?:\\\\\\\\)?\n",
    "        ([a-zA-Z0-9]+)\n",
    "        (\\.[a-zA-Z0-9]+)\n",
    "        (\\.[a-zA-Z0-9]+)*\n",
    "        (/[a-zA-Z0-9]+)*\n",
    "    \"\"\", re.VERBOSE|re.IGNORECASE)\n",
    "    space_regex = re.compile(r\"\\s+\")\n",
    "\n",
    "    line = url_regex.sub(r\"\", line)\n",
    "    line = space_regex.sub(r\"\", line)\n",
    "\n",
    "    return line\n",
    "\n",
    "def segment(sen_raw) -> list: # 对一个句子分词并返回一个链表\n",
    "    stopwords = [line.strip('\\n') for line in open('cutword.dat', 'r').readlines()] # 读入过滤列表\n",
    "    sen = []\n",
    "    try:\n",
    "        sen = jieba.lcut(sen_raw)\n",
    "    except:\n",
    "        pass\n",
    "    sen = [i for i in sen if i not in stopwords] # 跳过过滤表中的项目\n",
    "    return sen\n",
    "\n",
    "data = [regex_filter(line.strip('\\n')) for line in open('corpus10000.gbk', 'r').readlines()] # 读入整个文件，以链表形式储存，去掉换行符\n",
    "\n",
    "sentences = [segment(i) for i in data]\n",
    "# print(sentences) # 观察分词情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 训练词向量\n",
    "- 导入gensim\n",
    "- 训练模型\n",
    "- 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "s\n2020-07-22 07:01:17,510 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:17,511 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:17,516 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:17,517 : INFO : EPOCH - 14 : training on 651856 raw words (565427 effective words) took 0.4s, 1585284 effective words/s\n2020-07-22 07:01:17,902 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:17,906 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:17,908 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:17,908 : INFO : EPOCH - 15 : training on 651856 raw words (565292 effective words) took 0.4s, 1456393 effective words/s\n2020-07-22 07:01:18,265 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:18,267 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:18,269 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:18,270 : INFO : EPOCH - 16 : training on 651856 raw words (565561 effective words) took 0.4s, 1574880 effective words/s\n2020-07-22 07:01:18,612 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:18,616 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:18,618 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:18,618 : INFO : EPOCH - 17 : training on 651856 raw words (565509 effective words) took 0.3s, 1640240 effective words/s\n2020-07-22 07:01:18,966 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:18,967 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:18,970 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:18,971 : INFO : EPOCH - 18 : training on 651856 raw words (564907 effective words) took 0.3s, 1617927 effective words/s\n2020-07-22 07:01:19,326 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:19,330 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:19,331 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:19,332 : INFO : EPOCH - 19 : training on 651856 raw words (565482 effective words) took 0.4s, 1578802 effective words/s\n2020-07-22 07:01:19,671 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:19,672 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:19,677 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:19,678 : INFO : EPOCH - 20 : training on 651856 raw words (565234 effective words) took 0.3s, 1650237 effective words/s\n2020-07-22 07:01:20,045 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:20,046 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:20,047 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:20,047 : INFO : EPOCH - 21 : training on 651856 raw words (565142 effective words) took 0.4s, 1540803 effective words/s\n2020-07-22 07:01:20,390 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:20,391 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:20,396 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:20,396 : INFO : EPOCH - 22 : training on 651856 raw words (565155 effective words) took 0.3s, 1635453 effective words/s\n2020-07-22 07:01:20,734 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:20,737 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:20,744 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:20,745 : INFO : EPOCH - 23 : training on 651856 raw words (565342 effective words) took 0.3s, 1635726 effective words/s\n2020-07-22 07:01:21,087 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:21,088 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:21,093 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:21,094 : INFO : EPOCH - 24 : training on 651856 raw words (565419 effective words) took 0.3s, 1632067 effective words/s\n2020-07-22 07:01:21,436 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:21,442 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:21,446 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:21,447 : INFO : EPOCH - 25 : training on 651856 raw words (565585 effective words) took 0.4s, 1614957 effective words/s\n2020-07-22 07:01:21,448 : INFO : training on a 16296400 raw words (14132516 effective words) took 8.9s, 1579086 effective words/s\n2020-07-22 07:01:21,460 : INFO : saving Word2Vec object under CBOW_w3_s100_m1.model, separately None\n2020-07-22 07:01:21,461 : INFO : not storing attribute vectors_norm\n2020-07-22 07:01:21,462 : INFO : not storing attribute cum_table\n2020-07-22 07:01:21,893 : INFO : saved CBOW_w3_s100_m1.model\n2020-07-22 07:01:21,894 : INFO : collecting all words and their counts\n2020-07-22 07:01:21,895 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n2020-07-22 07:01:21,983 : INFO : collected 34998 word types from a corpus of 651856 raw words and 10000 sentences\n2020-07-22 07:01:21,983 : INFO : Loading a fresh vocabulary\n2020-07-22 07:01:22,067 : INFO : effective_min_count=1 retains 34998 unique words (100% of original 34998, drops 0)\n2020-07-22 07:01:22,068 : INFO : effective_min_count=1 leaves 651856 word corpus (100% of original 651856, drops 0)\n2020-07-22 07:01:22,152 : INFO : deleting the raw counts dictionary of 34998 items\n2020-07-22 07:01:22,153 : INFO : sample=0.001 downsamples 36 most-common words\n2020-07-22 07:01:22,153 : INFO : downsampling leaves estimated 565278 word corpus (86.7% of prior 651856)\n2020-07-22 07:01:22,226 : INFO : estimated required memory for 34998 words and 100 dimensions: 45497400 bytes\n2020-07-22 07:01:22,227 : INFO : resetting layer weights\n2020-07-22 07:01:27,800 : INFO : training model with 3 workers on 34998 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=6\n2020-07-22 07:01:28,810 : INFO : EPOCH 1 - PROGRESS: at 48.51% examples, 374714 words/s, in_qsize 6, out_qsize 0\n2020-07-22 07:01:29,239 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:29,251 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:29,287 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:29,288 : INFO : EPOCH - 1 : training on 651856 raw words (565276 effective words) took 1.5s, 380812 effective words/s\n2020-07-22 07:01:30,291 : INFO : EPOCH 2 - PROGRESS: at 47.34% examples, 368738 words/s, in_qsize 5, out_qsize 0\n2020-07-22 07:01:30,729 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:30,742 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:30,755 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:30,756 : INFO : EPOCH - 2 : training on 651856 raw words (565271 effective words) took 1.5s, 385663 effective words/s\n2020-07-22 07:01:31,764 : INFO : EPOCH 3 - PROGRESS: at 48.51% examples, 375780 words/s, in_qsize 5, out_qsize 0\n2020-07-22 07:01:32,194 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:32,205 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:32,219 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:32,220 : INFO : EPOCH - 3 : training on 651856 raw words (565168 effective words) took 1.5s, 386712 effective words/s\n2020-07-22 07:01:33,232 : INFO : EPOCH 4 - PROGRESS: at 48.51% examples, 374366 words/s, in_qsize 6, out_qsize 0\n2020-07-22 07:01:33,681 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:33,687 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:33,726 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:33,727 : INFO : EPOCH - 4 : training on 651856 raw words (565300 effective words) took 1.5s, 375864 effective words/s\n2020-07-22 07:01:34,750 : INFO : EPOCH 5 - PROGRESS: at 47.34% examples, 361701 words/s, in_qsize 5, out_qsize 0\n2020-07-22 07:01:35,161 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:35,192 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:35,216 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:35,217 : INFO : EPOCH - 5 : training on 651856 raw words (565181 effective words) took 1.5s, 380140 effective words/s\n2020-07-22 07:01:36,233 : INFO : EPOCH 6 - PROGRESS: at 48.51% examples, 373253 words/s, in_qsize 5, out_qsize 0\n2020-07-22 07:01:36,669 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:36,672 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:36,692 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:36,693 : INFO : EPOCH - 6 : training on 651856 raw words (565501 effective words) took 1.5s, 384023 effective words/s\n2020-07-22 07:01:37,697 : INFO : EPOCH 7 - PROGRESS: at 46.24% examples, 360899 words/s, in_qsize 5, out_qsize 0\n2020-07-22 07:01:38,144 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:38,151 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:38,157 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:38,158 : INFO : EPOCH - 7 : training on 651856 raw words (565467 effective words) took 1.5s, 386755 effective words/s\n2020-07-22 07:01:39,205 : INFO : EPOCH 8 - PROGRESS: at 49.62% examples, 369779 words/s, in_qsize 5, out_qsize 0\n2020-07-22 07:01:39,575 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:39,592 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:39,607 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:39,607 : INFO : EPOCH - 8 : training on 651856 raw words (565225 effective words) took 1.4s, 390855 effective words/s\n2020-07-22 07:01:40,623 : INFO : EPOCH 9 - PROGRESS: at 48.51% examples, 373137 words/s, in_qsize 6, out_qsize 0\n2020-07-22 07:01:41,045 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:41,046 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:41,050 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:41,051 : INFO : EPOCH - 9 : training on 651856 raw words (565322 effective words) took 1.4s, 392279 effective words/s\n2020-07-22 07:01:42,064 : INFO : EPOCH 10 - PROGRESS: at 47.34% examples, 366206 words/s, in_qsize 5, out_qsize 0\n2020-07-22 07:01:42,486 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:42,487 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:42,501 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:42,502 : INFO : EPOCH - 10 : training on 651856 raw words (565417 effective words) took 1.4s, 390618 effective words/s\n2020-07-22 07:01:43,514 : INFO : EPOCH 11 - PROGRESS: at 48.51% examples, 374106 words/s, in_qsize 5, out_qsize 0\n2020-07-22 07:01:43,969 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:43,975 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:43,991 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:43,992 : INFO : EPOCH - 11 : training on 651856 raw words (565404 effective words) took 1.5s, 380306 effective words/s\n2020-07-22 07:01:44,996 : INFO : EPOCH 12 - PROGRESS: at 48.51% examples, 377071 words/s, in_qsize 5, out_qsize 0\n2020-07-22 07:01:45,410 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:45,419 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:45,441 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:45,442 : INFO : EPOCH - 12 : training on 651856 raw words (565266 effective words) took 1.4s, 390363 effective words/s\n2020-07-22 07:01:46,476 : INFO : EPOCH 13 - PROGRESS: at 49.62% examples, 374274 words/s, in_qsize 6, out_qsize 0\n2020-07-22 07:01:46,832 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:46,852 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:46,863 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:46,864 : INFO : EPOCH - 13 : training on 651856 raw words (565086 effective words) took 1.4s, 398316 effective words/s\n2020-07-22 07:01:47,883 : INFO : EPOCH 14 - PROGRESS: at 47.34% examples, 363138 words/s, in_qsize 5, out_qsize 0\n2020-07-22 07:01:48,289 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:48,309 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:48,327 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:48,328 : INFO : EPOCH - 14 : training on 651856 raw words (565168 effective words) took 1.5s, 386638 effective words/s\n2020-07-22 07:01:49,352 : INFO : EPOCH 15 - PROGRESS: at 49.62% examples, 378253 words/s, in_qsize 5, out_qsize 0\n2020-07-22 07:01:49,742 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:49,761 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:49,770 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:49,771 : INFO : EPOCH - 15 : training on 651856 raw words (564935 effective words) took 1.4s, 392719 effective words/s\n2020-07-22 07:01:50,821 : INFO : EPOCH 16 - PROGRESS: at 49.62% examples, 368654 words/s, in_qsize 6, out_qsize 0\n2020-07-22 07:01:51,174 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:51,182 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:51,206 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:51,207 : INFO : EPOCH - 16 : training on 651856 raw words (565191 effective words) took 1.4s, 394294 effective words/s\n2020-07-22 07:01:52,265 : INFO : EPOCH 17 - PROGRESS: at 49.62% examples, 366247 words/s, in_qsize 5, out_qsize 0\n2020-07-22 07:01:52,627 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:52,628 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:52,644 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:52,645 : INFO : EPOCH - 17 : training on 651856 raw words (565147 effective words) took 1.4s, 393859 effective words/s\n2020-07-22 07:01:53,689 : INFO : EPOCH 18 - PROGRESS: at 49.62% examples, 370643 words/s, in_qsize 6, out_qsize 0\n2020-07-22 07:01:54,046 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:54,071 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:54,073 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:54,073 : INFO : EPOCH - 18 : training on 651856 raw words (565158 effective words) took 1.4s, 396661 effective words/s\n2020-07-22 07:01:55,095 : INFO : EPOCH 19 - PROGRESS: at 49.62% examples, 379084 words/s, in_qsize 5, out_qsize 0\n2020-07-22 07:01:55,478 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:55,479 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:55,505 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:55,505 : INFO : EPOCH - 19 : training on 651856 raw words (565472 effective words) took 1.4s, 395678 effective words/s\n2020-07-22 07:01:56,514 : INFO : EPOCH 20 - PROGRESS: at 48.51% examples, 375022 words/s, in_qsize 5, out_qsize 0\n2020-07-22 07:01:56,922 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:56,930 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:56,937 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:56,938 : INFO : EPOCH - 20 : training on 651856 raw words (565126 effective words) took 1.4s, 395176 effective words/s\n2020-07-22 07:01:57,952 : INFO : EPOCH 21 - PROGRESS: at 46.24% examples, 357056 words/s, in_qsize 5, out_qsize 0\n2020-07-22 07:01:58,388 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:58,406 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:58,408 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:58,409 : INFO : EPOCH - 21 : training on 651856 raw words (565316 effective words) took 1.5s, 384928 effective words/s\n2020-07-22 07:01:59,424 : INFO : EPOCH 22 - PROGRESS: at 47.34% examples, 365363 words/s, in_qsize 5, out_qsize 0\n2020-07-22 07:01:59,842 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:01:59,852 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:01:59,879 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:01:59,880 : INFO : EPOCH - 22 : training on 651856 raw words (565182 effective words) took 1.5s, 385406 effective words/s\n2020-07-22 07:02:00,943 : INFO : EPOCH 23 - PROGRESS: at 49.62% examples, 364057 words/s, in_qsize 6, out_qsize 0\n2020-07-22 07:02:01,321 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:02:01,332 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:02:01,351 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:02:01,353 : INFO : EPOCH - 23 : training on 651856 raw words (565088 effective words) took 1.5s, 384573 effective words/s\n2020-07-22 07:02:02,358 : INFO : EPOCH 24 - PROGRESS: at 49.62% examples, 385016 words/s, in_qsize 5, out_qsize 0\n2020-07-22 07:02:02,730 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:02:02,757 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:02:02,764 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:02:02,764 : INFO : EPOCH - 24 : training on 651856 raw words (565317 effective words) took 1.4s, 401219 effective words/s\n2020-07-22 07:02:03,790 : INFO : EPOCH 25 - PROGRESS: at 49.62% examples, 377562 words/s, in_qsize 5, out_qsize 0\n2020-07-22 07:02:04,168 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-07-22 07:02:04,171 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-07-22 07:02:04,189 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-07-22 07:02:04,191 : INFO : EPOCH - 25 : training on 651856 raw words (565275 effective words) took 1.4s, 396922 effective words/s\n2020-07-22 07:02:04,192 : INFO : training on a 16296400 raw words (14131259 effective words) took 36.4s, 388314 effective words/s\n2020-07-22 07:02:04,205 : INFO : saving Word2Vec object under SKIP_GRAM_w6_s100_m1.model, separately None\n2020-07-22 07:02:04,206 : INFO : not storing attribute vectors_norm\n2020-07-22 07:02:04,206 : INFO : not storing attribute cum_table\n2020-07-22 07:02:04,642 : INFO : saved SKIP_GRAM_w6_s100_m1.model\n"
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "import logging \n",
    "\n",
    "# 生成日志\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level = logging.INFO) \n",
    "# 词向量维度100，window为5，CBOW模型\n",
    "model = word2vec.Word2Vec(sentences, min_count=1,iter=25)\n",
    "# 保存模型\n",
    "model.save(\"CBOW_w5_s100_m1.model\")\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, min_count=2,iter=25)\n",
    "model.save(\"CBOW_w5_s100_m2.model\")\n",
    "model = word2vec.Word2Vec(sentences, min_count=1,iter=25, size = 200)\n",
    "model.save(\"CBOW_w5_s200_m1.model\")\n",
    "model = word2vec.Word2Vec(sentences, min_count=1,iter=25, window = 3)\n",
    "model.save(\"CBOW_w3_s100_m1.model\")\n",
    "model = word2vec.Word2Vec(sentences, min_count=1,iter=25, window = 6, sg = 1)\n",
    "model.save(\"SKIP_GRAM_w6_s100_m1.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 使用训练好的词向量模型\n",
    "- 计算词之间的相似性\n",
    "- 输出词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2020-07-22 07:02:53,680 : INFO : loading Word2Vec object from CBOW_w5_s100_m1.model\n2020-07-22 07:02:53,933 : INFO : loading wv recursively from CBOW_w5_s100_m1.model.wv.* with mmap=None\n2020-07-22 07:02:53,933 : INFO : setting ignored attribute vectors_norm to None\n2020-07-22 07:02:53,934 : INFO : loading vocabulary recursively from CBOW_w5_s100_m1.model.vocabulary.* with mmap=None\n2020-07-22 07:02:53,935 : INFO : loading trainables recursively from CBOW_w5_s100_m1.model.trainables.* with mmap=None\n2020-07-22 07:02:53,935 : INFO : setting ignored attribute cum_table to None\n2020-07-22 07:02:53,936 : INFO : loaded CBOW_w5_s100_m1.model\n2020-07-22 07:02:53,996 : INFO : loading Word2Vec object from CBOW_w5_s100_m2.model\n2020-07-22 07:02:54,117 : INFO : loading wv recursively from CBOW_w5_s100_m2.model.wv.* with mmap=None\n2020-07-22 07:02:54,117 : INFO : setting ignored attribute vectors_norm to None\n2020-07-22 07:02:54,118 : INFO : loading vocabulary recursively from CBOW_w5_s100_m2.model.vocabulary.* with mmap=None\n2020-07-22 07:02:54,118 : INFO : loading trainables recursively from CBOW_w5_s100_m2.model.trainables.* with mmap=None\n2020-07-22 07:02:54,119 : INFO : setting ignored attribute cum_table to None\n2020-07-22 07:02:54,120 : INFO : loaded CBOW_w5_s100_m2.model\n2020-07-22 07:02:54,155 : INFO : loading Word2Vec object from CBOW_w3_s100_m1.model\n2020-07-22 07:02:54,406 : INFO : loading wv recursively from CBOW_w3_s100_m1.model.wv.* with mmap=None\n2020-07-22 07:02:54,407 : INFO : setting ignored attribute vectors_norm to None\n2020-07-22 07:02:54,407 : INFO : loading vocabulary recursively from CBOW_w3_s100_m1.model.vocabulary.* with mmap=None\n2020-07-22 07:02:54,408 : INFO : loading trainables recursively from CBOW_w3_s100_m1.model.trainables.* with mmap=None\n2020-07-22 07:02:54,409 : INFO : setting ignored attribute cum_table to None\n2020-07-22 07:02:54,409 : INFO : loaded CBOW_w3_s100_m1.model\n2020-07-22 07:02:54,462 : INFO : loading Word2Vec object from CBOW_w5_s200_m1.model\n2020-07-22 07:02:54,838 : INFO : loading wv recursively from CBOW_w5_s200_m1.model.wv.* with mmap=None\n2020-07-22 07:02:54,838 : INFO : setting ignored attribute vectors_norm to None\n2020-07-22 07:02:54,839 : INFO : loading vocabulary recursively from CBOW_w5_s200_m1.model.vocabulary.* with mmap=None\n2020-07-22 07:02:54,839 : INFO : loading trainables recursively from CBOW_w5_s200_m1.model.trainables.* with mmap=None\n2020-07-22 07:02:54,840 : INFO : setting ignored attribute cum_table to None\n2020-07-22 07:02:54,840 : INFO : loaded CBOW_w5_s200_m1.model\n2020-07-22 07:02:54,894 : INFO : loading Word2Vec object from SKIP_GRAM_w6_s100_m1.model\n2020-07-22 07:02:55,159 : INFO : loading wv recursively from SKIP_GRAM_w6_s100_m1.model.wv.* with mmap=None\n2020-07-22 07:02:55,160 : INFO : setting ignored attribute vectors_norm to None\n2020-07-22 07:02:55,161 : INFO : loading vocabulary recursively from SKIP_GRAM_w6_s100_m1.model.vocabulary.* with mmap=None\n2020-07-22 07:02:55,161 : INFO : loading trainables recursively from SKIP_GRAM_w6_s100_m1.model.trainables.* with mmap=None\n2020-07-22 07:02:55,161 : INFO : setting ignored attribute cum_table to None\n2020-07-22 07:02:55,162 : INFO : loaded SKIP_GRAM_w6_s100_m1.model\n2020-07-22 07:02:55,240 : INFO : precomputing L2-norms of word weight vectors\n2020-07-22 07:02:55,373 : INFO : precomputing L2-norms of word weight vectors\n2020-07-22 07:02:55,396 : INFO : precomputing L2-norms of word weight vectors\nmodel 1:\n\"物理\"和\"化学\"间的相似度为: 0.5921306610107422\n\"早餐\"和\"早饭\"间的相似度为: 0.6128503680229187\n\"光明\"和\"黑暗\"间的相似度为: 0.6717828512191772\n\"聪明\"和\"愚蠢\"间的相似度为: 0.33986908197402954\n和大学最相关的10个词为: 高中 距离为: 0.6740305423736572\n和大学最相关的10个词为: 学校 距离为: 0.6036063432693481\n和大学最相关的10个词为: 实验班 距离为: 0.5481260418891907\n和大学最相关的10个词为: 本科 距离为: 0.5477038621902466\n和大学最相关的10个词为: 一所 距离为: 0.5416812896728516\n和大学最相关的10个词为: 清华 距离为: 0.5373642444610596\n和大学最相关的10个词为: 城市 距离为: 0.5364798307418823\n和大学最相关的10个词为: 考 距离为: 0.5333002805709839\n和大学最相关的10个词为: 农村 距离为: 0.5123885869979858\n和大学最相关的10个词为: 初中 距离为: 0.5056415796279907\nmodel 2:\n\"物理\"和\"化学\"间的相似度为: 0.6065905690193176\n\"早餐\"和\"早饭\"间的相似度为: 0.4863530993461609\n\"光明\"和\"黑暗\"间的相似度为: 0.623782753944397\n\"聪明\"和\"愚蠢\"间的相似度为: 0.29866844415664673\n和大学最相关的10个词为: 高中 距离为: 0.6620303392410278\n和大学最相关的10个词为: 学校 距离为: 0.6070740222930908\n和大学最相关的10个词为: 本科 距离为: 0.549144446849823\n和大学最相关的10个词为: 初中 距离为: 0.5486642122268677\n和大学最相关的10个词为: 实验室 距离为: 0.5343855619430542\n和大学最相关的10个词为: 城市 距离为: 0.5243170261383057\n和大学最相关的10个词为: 考 距离为: 0.5152467489242554\n和大学最相关的10个词为: 四年 距离为: 0.5130516290664673\n和大学最相关的10个词为: 同 距离为: 0.5115454196929932\n和大学最相关的10个词为: 读 距离为: 0.5082521438598633\nmodel 3:\n\"物理\"和\"化学\"间的相似度为: 0.5486980676651001\n\"早餐\"和\"早饭\"间的相似度为: 0.6280509233474731\n\"光明\"和\"黑暗\"间的相似度为: 0.7608284950256348\n\"聪明\"和\"愚蠢\"间的相似度为: 0.38678431510925293\n和大学最相关的10个词为: 高中 距离为: 0.6764050126075745\n和大学最相关的10个词为: 学校 距离为: 0.645466148853302\n和大学最相关的10个词为: 本科 距离为: 0.6103152632713318\n和大学最相关的10个词为: 初中 距离为: 0.6030247211456299\n和大学最相关的10个词为: 实验室 距离为:2020-07-22 07:02:55,428 : INFO : precomputing L2-norms of word weight vectors\n2020-07-22 07:02:55,487 : INFO : precomputing L2-norms of word weight vectors\n0.5999265313148499\n和大学最相关的10个词为: 清华 距离为: 0.5712719559669495\n和大学最相关的10个词为: 研究生 距离为: 0.5545953512191772\n和大学最相关的10个词为: 实验班 距离为: 0.5453998446464539\n和大学最相关的10个词为: 国外 距离为: 0.5408526659011841\n和大学最相关的10个词为: 考 距离为: 0.532413125038147\nmodel 4:\n\"物理\"和\"化学\"间的相似度为: 0.5585203170776367\n\"早餐\"和\"早饭\"间的相似度为: 0.5584297180175781\n\"光明\"和\"黑暗\"间的相似度为: 0.6451290845870972\n\"聪明\"和\"愚蠢\"间的相似度为: 0.3525882959365845\n和大学最相关的10个词为: 高中 距离为: 0.6634707450866699\n和大学最相关的10个词为: 学校 距离为: 0.5923126935958862\n和大学最相关的10个词为: 本科 距离为: 0.5749603509902954\n和大学最相关的10个词为: 初中 距离为: 0.569144606590271\n和大学最相关的10个词为: 实验室 距离为: 0.541892409324646\n和大学最相关的10个词为: 国外 距离为: 0.5347803235054016\n和大学最相关的10个词为: 城市 距离为: 0.534041166305542\n和大学最相关的10个词为: 一所 距离为: 0.5226634740829468\n和大学最相关的10个词为: 来念 距离为: 0.5216158032417297\n和大学最相关的10个词为: 毕业 距离为: 0.510660707950592\nmodel 5:\n\"物理\"和\"化学\"间的相似度为: 0.6509181261062622\n\"早餐\"和\"早饭\"间的相似度为: 0.5529741048812866\n\"光明\"和\"黑暗\"间的相似度为: 0.6909791827201843\n\"聪明\"和\"愚蠢\"间的相似度为: 0.4942142069339752\n和大学最相关的10个词为: 极烂 距离为: 0.677933931350708\n和大学最相关的10个词为: 法律硕士 距离为: 0.663931131362915\n和大学最相关的10个词为: 爱玩 距离为: 0.6373225450515747\n和大学最相关的10个词为: 哭鼻子 距离为: 0.6271693706512451\n和大学最相关的10个词为: 不入流 距离为: 0.6181372404098511\n和大学最相关的10个词为: 考个 距离为: 0.6142526865005493\n和大学最相关的10个词为: 挺帅 距离为: 0.6089746952056885\n和大学最相关的10个词为: 安家费 距离为: 0.5960806608200073\n和大学最相关的10个词为: 几成 距离为: 0.5951089859008789\n和大学最相关的10个词为: 理工大学 距离为: 0.5919551849365234\n"
    }
   ],
   "source": [
    "def load(name):\n",
    "    return word2vec.Word2Vec.load(name)\n",
    "\n",
    "def similar(model, word1, word2):\n",
    "    print('\\\"{0}\\\"和\\\"{1}\\\"间的相似度为: {2}'.format(word1, word2, model.similarity(word1, word2)))\n",
    "\n",
    "def test(model, i):\n",
    "    print('model {0}:'.format(i))\n",
    "    similar(model, '物理','化学')\n",
    "    similar(model, '早餐','早饭')\n",
    "    # similar(model, '马虎','粗心') # 相似度-0.01 检查一下词频可以发现“马虎”只出现过一次，可以想见这结果不太可靠\n",
    "    # similar(model, '寒冷','炎热') # '炎热'不在字典中\n",
    "    similar(model, '光明','黑暗')\n",
    "    similar(model, '聪明','愚蠢')\n",
    "    for key in model.wv.most_similar('大学', topn = 10):\n",
    "        print('和'+'大学'+'最相关的10个词为:',key[0],'距离为:', key[1])\n",
    "        \n",
    "# 加载训练好的模型\n",
    "test_suite = [load(\"CBOW_w5_s100_m1.model\"), load(\"CBOW_w5_s100_m2.model\"), load(\"CBOW_w3_s100_m1.model\"), load(\"CBOW_w5_s200_m1.model\"), load(\"SKIP_GRAM_w6_s100_m1.model\")]\n",
    "for i, model in enumerate(test_suite):\n",
    "    test(model, i+1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 分析：小语料下模型参数的选择\n",
    "\n",
    "小语料下Skip-Gram模型表现不佳，CBOW模型表现较好\n",
    "\n",
    "小语料下，window选择小一些能够提升模型准确度，size变化对结果影响不太大\n",
    "\n",
    "最小词频提高能够筛选掉部分罕见词，但语料过小，提高最小词频会删去可能并非罕见词的词语，使本就不太丰富的信息进一步减少"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 更大语料下的word2vec\n",
    "\n",
    "采用中文wiki数据500M\n",
    "\n",
    "### 预处理：\n",
    "- 将繁体字全部转换为简体字(opencc)\n",
    "- 用正则表达式去除数据中的标签和空行(Python跑了一个多钟头都没跑完，换成C++了)\n",
    "\n",
    "--------------------------\n",
    "\n",
    "预处理C++源码：\n",
    "```c++\n",
    "#include <iostream>\n",
    "#include <fstream>\n",
    "#include <regex>\n",
    "#include <cctype>\n",
    "\n",
    "using namespace std;\n",
    "\n",
    "bool is_empty_line(string line) {\n",
    "    for (auto &c : line) {\n",
    "        if (!isspace(c)) return false;\n",
    "    }\n",
    "    return true;\n",
    "}\n",
    "\n",
    "int main(void) {\n",
    "    fstream fin(\"zhwiki_500.txt\", ios::in), fout(\"zhwiki_500_processed.txt\", ios::out);\n",
    "    char line[100000];\n",
    "    while (!fin.eof()) {\n",
    "        fin.getline(line, 100000);\n",
    "        \n",
    "        string str = regex_replace(line, regex(\"<.*>\"), \"\"); // 删除标签\n",
    "        if (is_empty_line(str)) continue;                    // 空行跳过\n",
    "        fout << line << endl;\n",
    "    }\n",
    "    fin.close();\n",
    "    fout.close();\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "------------------------\n",
    "\n",
    "最终语料句子共180万句，约500M(*结果分词阶段跑到内存溢出*\n",
    "\n",
    "*考虑到内存占用和训练时间的关系，只选用了前60万行，估计约160M，共8000+万字*\n",
    "\n",
    "总训练时间：约90分钟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import jieba\n",
    "import logging \n",
    "\n",
    "def segment(sen_raw) -> list: # 对一个句子分词并返回一个链表\n",
    "    sen = []\n",
    "    try:\n",
    "        sen = jieba.lcut(sen_raw)\n",
    "    except:\n",
    "        pass\n",
    "    return sen\n",
    "\n",
    "sub = [line.strip('\\n') for line in open('zhwiki_500_processed.txt', 'r', errors = 'ignore').readlines()]\n",
    "sens = [segment(i) for i in sub[:600000]] # 内存装不下，只取前60万行\n",
    "# 生成日志\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level = logging.INFO) \n",
    "model = word2vec.Word2Vec(sens, min_count=2,iter=25, window = 5, size = 500)\n",
    "model.save(\"wiki_CBOW_w5_s500_m2.model\")\n",
    "model = word2vec.Word2Vec(sens, min_count=2,iter=25, window = 5, size = 500, sg = 1)\n",
    "model.save(\"wiki_SKIP_GRAM_w5_s500_m2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2020-07-22 08:35:29,691 : INFO : loading Word2Vec object from wiki_CBOW_w5_s500_m2.model\n2020-07-22 08:35:30,446 : INFO : loading wv recursively from wiki_CBOW_w5_s500_m2.model.wv.* with mmap=None\n2020-07-22 08:35:30,447 : INFO : loading vectors from wiki_CBOW_w5_s500_m2.model.wv.vectors.npy with mmap=None\n2020-07-22 08:35:31,796 : INFO : setting ignored attribute vectors_norm to None\n2020-07-22 08:35:31,797 : INFO : loading vocabulary recursively from wiki_CBOW_w5_s500_m2.model.vocabulary.* with mmap=None\n2020-07-22 08:35:31,798 : INFO : loading trainables recursively from wiki_CBOW_w5_s500_m2.model.trainables.* with mmap=None\n2020-07-22 08:35:31,798 : INFO : loading syn1neg from wiki_CBOW_w5_s500_m2.model.trainables.syn1neg.npy with mmap=None\n2020-07-22 08:35:32,500 : INFO : setting ignored attribute cum_table to None\n2020-07-22 08:35:32,501 : INFO : loaded wiki_CBOW_w5_s500_m2.model\n2020-07-22 08:35:33,250 : INFO : loading Word2Vec object from wiki_SKIP_GRAM_w5_s500_m2.model\n2020-07-22 08:35:34,024 : INFO : loading wv recursively from wiki_SKIP_GRAM_w5_s500_m2.model.wv.* with mmap=None\n2020-07-22 08:35:34,025 : INFO : loading vectors from wiki_SKIP_GRAM_w5_s500_m2.model.wv.vectors.npy with mmap=None\n2020-07-22 08:35:35,416 : INFO : setting ignored attribute vectors_norm to None\n2020-07-22 08:35:35,416 : INFO : loading vocabulary recursively from wiki_SKIP_GRAM_w5_s500_m2.model.vocabulary.* with mmap=None\n2020-07-22 08:35:35,417 : INFO : loading trainables recursively from wiki_SKIP_GRAM_w5_s500_m2.model.trainables.* with mmap=None\n2020-07-22 08:35:35,417 : INFO : loading syn1neg from wiki_SKIP_GRAM_w5_s500_m2.model.trainables.syn1neg.npy with mmap=None\n2020-07-22 08:35:36,816 : INFO : setting ignored attribute cum_table to None\n2020-07-22 08:35:36,817 : INFO : loaded wiki_SKIP_GRAM_w5_s500_m2.model\n2020-07-22 08:35:38,087 : INFO : precomputing L2-norms of word weight vectors\nmodel 1:\n\"物理\"和\"化学\"间的相似度为: 0.5170843005180359\n\"早餐\"和\"早饭\"间的相似度为: 0.39237600564956665\n\"寒冷\"和\"炎热\"间的相似度为: 0.6497912406921387\n\"光明\"和\"黑暗\"间的相似度为: 0.34593698382377625\n\"聪明\"和\"愚蠢\"间的相似度为: 0.4076206386089325\n2020-07-22 08:35:39,538 : INFO : precomputing L2-norms of word weight vectors\n在\"大学 高中 初中 中学 小学 教室 幼儿园\"中，不合群的词是教室\n和大学最相关的10个词为: 学院 距离为: 0.5902684926986694\n和大学最相关的10个词为: 理工大学 距离为: 0.586166501045227\n和大学最相关的10个词为: 大学教授 距离为: 0.581559419631958\n和大学最相关的10个词为: 高等学府 距离为: 0.5521895885467529\n和大学最相关的10个词为: 大学校长 距离为: 0.548029899597168\n和大学最相关的10个词为: 大学校园 距离为: 0.5397053360939026\n和大学最相关的10个词为: 州立大学 距离为: 0.5324269533157349\n和大学最相关的10个词为: 理工学院 距离为: 0.5300482511520386\n和大学最相关的10个词为: 文理学院 距离为: 0.5229028463363647\n和大学最相关的10个词为: 国立大学 距离为: 0.5224639177322388\n和中国最相关的10个词为: 日本 距离为: 0.4946754574775696\n和中国最相关的10个词为: 中华人民共和国 距离为: 0.49272090196609497\n和中国最相关的10个词为: 我国 距离为: 0.4622647166252136\n和中国最相关的10个词为: 台湾 距离为: 0.4558970034122467\n和中国最相关的10个词为: 欧美 距离为: 0.41189348697662354\n和中国最相关的10个词为: 韩国 距离为: 0.40880662202835083\n和中国最相关的10个词为: 东亚 距离为: 0.4070664346218109\n和中国最相关的10个词为: 香港 距离为: 0.40122562646865845\n和中国最相关的10个词为: 礁层 距离为: 0.40053611993789673\n和中国最相关的10个词为: 西方 距离为: 0.39891618490219116\n和罗马帝国最相关的10个词为: 罗马 距离为: 0.6273125410079956\n和罗马帝国最相关的10个词为: 拜占庭 距离为: 0.5539195537567139\n和罗马帝国最相关的10个词为: 奥斯曼帝国 距离为: 0.5394849181175232\n和罗马帝国最相关的10个词为: 帝国 距离为: 0.5380226969718933\n和罗马帝国最相关的10个词为: 日耳曼人 距离为: 0.5278815031051636\n和罗马帝国最相关的10个词为: 哥德人 距离为: 0.5242370367050171\n和罗马帝国最相关的10个词为: 罗马城 距离为: 0.5114766955375671\n和罗马帝国最相关的10个词为: 高卢 距离为: 0.5015509128570557\n和罗马帝国最相关的10个词为: 君士坦丁堡 距离为: 0.49111080169677734\n和罗马帝国最相关的10个词为: 中世纪 距离为: 0.4735454320907593\nmodel 2:\n\"物理\"和\"化学\"间的相似度为: 0.5769115090370178\n\"早餐\"和\"早饭\"间的相似度为: 0.40617844462394714\n\"寒冷\"和\"炎热\"间的相似度为: 0.5446626543998718\n\"光明\"和\"黑暗\"间的相似度为: 0.32979387044906616\n\"聪明\"和\"愚蠢\"间的相似度为: 0.26912835240364075\n在\"大学 高中 初中 中学 小学 教室 幼儿园\"中，不合群的词是教室\n和大学最相关的10个词为: 私立 距离为: 0.5825138092041016\n和大学最相关的10个词为: University 距离为: 0.5757034420967102\n和大学最相关的10个词为: 学院 距离为: 0.5518354177474976\n和大学最相关的10个词为: 国立大学 距离为: 0.5387105941772461\n和大学最相关的10个词为: 理工大学 距离为: 0.5243420600891113\n和大学最相关的10个词为: 医科大学 距离为: 0.5218995213508606\n和大学最相关的10个词为: 外交系 距离为: 0.5207165479660034\n和大学最相关的10个词为: 医学院 距离为: 0.5167540311813354\n和大学最相关的10个词为: 研究生院 距离为: 0.5149999856948853\n和大学最相关的10个词为: 江西财经大学 距离为: 0.5147090554237366\n和中国最相关的10个词为: 大陆 距离为: 0.5950651168823242\n和中国最相关的10个词为: 中华人民共和国 距离为: 0.4614976942539215\n和中国最相关的10个词为: 盆景花卉 距离为: 0.4321647882461548\n和中国最相关的10个词为: 中国政府 距离为: 0.4264727830886841\n和中国最相关的10个词为: 李智楠 距离为: 0.42559683322906494\n和中国最相关的10个词为: 尚明轩 距离为: 0.42089638113975525\n和中国最相关的10个词为: 内地 距离为: 0.4184255301952362\n和中国最相关的10个词为: 贝宝 距离为: 0.4170694649219513\n和中国最相关的10个词为: 乐谷 距离为: 0.4152541756629944\n和中国最相关的10个词为: 精煤 距离为: 0.41318175196647644\n和罗马帝国最相关的10个词为: 神圣 距离为: 0.7140364646911621\n和罗马帝国最相关的10个词为: 罗马 距离为: 0.5797584652900696\n和罗马帝国最相关的10个词为: 拜占庭 距离为: 0.572625458240509\n和罗马帝国最相关的10个词为: 先宫 距离为: 0.5336763858795166\n和罗马帝国最相关的10个词为: 即屋 距离为: 0.5319394469261169\n和罗马帝国最相关的10个词为: 西吉斯 距离为: 0.5208421945571899\n和罗马帝国最相关的10个词为: 狄奥凡诺 距离为: 0.5201631784439087\n和罗马帝国最相关的10个词为: 陶芬 距离为: 0.5200348496437073\n和罗马帝国最相关的10个词为: 吉伯林 距离为: 0.5158476829528809\n和罗马帝国最相关的10个词为: 共治制 距离为: 0.5153853893280029\n"
    }
   ],
   "source": [
    "def test(model, i):\n",
    "    print('model {0}:'.format(i))\n",
    "    similar(model, '物理','化学')\n",
    "    similar(model, '早餐','早饭')\n",
    "    similar(model, '寒冷','炎热') \n",
    "    similar(model, '光明','黑暗')\n",
    "    similar(model, '聪明','愚蠢')\n",
    "    string = \"大学 高中 初中 中学 小学 教室 幼儿园\"\n",
    "    print('在\\\"'+string+'\\\"中，不合群的词是{}'.format(model.wv.doesnt_match(string.split())))\n",
    "    for key in model.wv.most_similar('大学', topn = 10):\n",
    "        print('和'+'大学'+'最相关的10个词为:',key[0],'距离为:', key[1])\n",
    "    for key in model.wv.most_similar('中国', topn = 10):\n",
    "        print('和'+'中国'+'最相关的10个词为:',key[0],'距离为:', key[1])\n",
    "    for key in model.wv.most_similar('罗马帝国', topn = 10):\n",
    "        print('和'+'罗马帝国'+'最相关的10个词为:',key[0],'距离为:', key[1])\n",
    "\n",
    "test_suite = [load(\"wiki_CBOW_w5_s500_m2.model\"), load(\"wiki_SKIP_GRAM_w5_s500_m2.model\")]\n",
    "\n",
    "for i, model in enumerate(test_suite):\n",
    "    test(model, i+1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 wiki语料训练结果分析\n",
    "\n",
    "可以明显看出模型训练后的效果明显提升，特别是SKIP-GRAM模型达到了可用的水准\n",
    "\n",
    "两个模型的不同特点在这里也有所体现，CBOW的结果更倾向于词性和功能相近的归到一类，而SKIP-GRAM则把意思上相关的归到一类\n",
    "\n",
    "取距离最接近的词，CBOW的结果通常可以直接替换原词，SKIP-GRAM的结果则经常是和原词有某种联系"
   ]
  }
 ]
}